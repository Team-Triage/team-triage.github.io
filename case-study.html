<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">
  <head>
    <meta charset="utf-8" />
    <title>Triage • Case Study</title>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
    <script
      src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
      type="text/javascript"
    ></script>
    <script src="/assets/scripts/collapseScript.js"></script>
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Inter:regular,500,600,700"
      media="all"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Work+Sans"
    />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/css?family=Nunito+Sans"
    />
    <meta
    name="image"
    property="og:image"
    content="assets/images/synapse-case-study.png"
  />
    <script type="text/javascript">
      WebFont.load({ google: { families: ["Inter:regular,500,600,700"] } });
    </script>
    <script type="text/javascript">
      !(function (o, c) {
        var n = c.documentElement,
          t = " w-mod-";
        (n.className += t + "js"),
          ("ontouchstart" in o ||
            (o.DocumentTouch && c instanceof DocumentTouch)) &&
            (n.className += t + "touch");
      })(window, document);
    </script>
    <link
      href="assets/images/logo-color.png"
      rel="shortcut icon"
      type="image/x-icon"
    />
    <link href="assets/images/logo-mono.png" rel="apple-touch-icon" />
    <script
      src="https://kit.fontawesome.com/d019875f94.js"
      crossorigin="anonymous"
    ></script>
    <meta
      name="image"
      property="og:image"
      content="assets/images/thumbnail.png"
    />
  </head>

  <body>
    <!--Navbar-->
    <div class="navigation-wrap" id="navbar">
      <div
        data-collapse="medium"
        data-animation="default"
        data-duration="400"
        role="banner"
        class="navigation w-nav"
      >
        <div class="navigation-container">
          <div class="navigation-left">
            <a
              href="/"
              aria-current="page"
              class="brand w-nav-brand w--current"
              aria-label="home"
            >
            <img
            src="assets/images/logo-color.png"
            alt=""
            class="template-logo"
            onmouseover="hover(this);"
            onmouseout="unhover(this);"
          />
            </a>

            <nav role="navigation" class="nav-menu w-nav-menu">
              <nav role="navigation" class="nav-menu w-nav-menu">
                <a href="./case-study.html" class="link-block w-inline-block">
                  <div>Case Study</div>
                </a>
                <a href="./team.html" class="link-block w-inline-block">
                  <div>Team</div>
                </a>
                <!-- <a href="./presentation.html" class="link-block w-inline-block">
                  <div>Presentation</div>
                </a> -->
              </nav>
            </nav>
          </div>
          <div class="navigation-right">
            <div class="login-buttons">
              <a
                href="https://github.com/Team-Triage"
                target="_blank"
              >
                <span style="color: #FFFFFF">
                  <i class="fab fa-github fa-lg"></i>
                </span>
              </a>
            </div>
          </div>
        </div>
        <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
      </div>
    </div>

    <!-- <div class="wrapper"></div> -->

    <div id="sidebar" class="toc"></div>

    <div class="section header">
      <article class="container case-study-container">
        <div class="hero-text-container">
          <h1 class="h1 centered">Triage - A Kafka Proxy</h1>
        </div>
        <div id="case-study">
          <br />
          <br />

          <!-- Introduction -->
          <div class="case-study-section">
            <h2 class="h2">Introduction</h2>

            <div class="case-study-subsection">
              <p>
                Triage is an open-source consumer proxy for Apache Kafka that solves head-of-line blocking 
                (HoLB) caused by poison pill messages and non-uniform latency. Once deployed, poison pill 
                messages will be identified and delivered to a dead letter store. By enabling additional
                consumer instances to consume messages, Triage uses parallelism to ensure that an unusually 
                slow message will not block the queue. 
              </p>
              <p>
                Our goal was to create a service that could deal with HoLB in a message queue while making
                it easy for consumer application developers to maintain their existing workflow with minimal changes. 
              </p>
              <p>
                This case study will begin by exploring the larger context of microservices and the role of 
                message queues in facilitating event-driven architectures. It will also describe some of the 
                basics regarding Kafka’s functionality and how HoLB can affect consumers, followed by an overview 
                of existing solutions. Finally, we’ll dive into the architecture of Triage, discuss important 
                technical decisions we made, and outline the key challenges we faced during the process. 
              </p>
            </div>
          </div>
            
          <!-- Section 1: Problem Domain Setup -->
          <div class="case-study-section">
            <h2 class="h2">Problem Domain Setup</h2>
            <div class="case-study-subsection">
              <h3>The World of Microservices</h3>
              <p>
                Over the last decade, microservices have become a popular architectural 
                choice for building applications. By one estimate from 2020, 63% of enterprises
                have adopted microservices, and many are satisfied with the tradeoffs [1].
                Decoupling services often leads to faster development time since work on 
                different services can be done in parallel. Additionally, many companies benefit
                from the ability to independently scale individual components of their architecture, 
                and this same decoupling makes it easier to isolate failures in a system. 
              </p>
              <p>
                Microservice architectures are flexible enough to allow different technologies and
                languages to communicate within the same system, creating a polyglot environment.
                This flexibility enables a multitude of different approaches for achieving reliable
                intra-system communication.
              </p>
              <p>
                Two common options are the request-response pattern and event-driven architecture (EDA).
                Although the latter is where our focus lies, it’s useful to have some context on the shift 
                toward EDAs.
              </p>
            </div>

            <div class="case-study-subsection">
              <h3>From Request-Response to Event-Driven Architecture</h3>

              <p>
                A typical request-response pattern is commonly used on the web, and that is no different
                from what we’re referring to here. For example, imagine a number of interconnected
                microservices. One of them sends a request to another and waits for a response. If any one
                of the services in this chain experiences lag or failure, a cascade of slowdowns moves
                throughout the entire system. 
              </p>

              <figure>
                <img
                  src="assets/images/case-study/req-res.gif"
                  class="case-study-image"
                />
              </figure>

              <p>
                In an EDA, however, the approach is centered around “events”, which can be thought of
                as any changes in state or notifications about a change. The key advantage is that each service 
                can operate without concern for the state of any other service - that is, they perform their 
                tasks without interacting with other services in the architecture. EDAs are often implemented using 
                message queues. Producers write events to the message queue, and consumers read events off of it. For
                example, imagine an online store - a producer application might detect that an order has been submitted 
                and write an “order” event to the queue. A consumer application could then see that order, dequeue it,
                and process it accordingly.
              </p>

              <figure>
                <img
                  src="assets/images/case-study/eda.gif"
                  class="case-study-image"
                />
              </figure>
            </div>
          </div>
          <!-- Section 2: Apache Kafka -->
          <div class="case-study-section">
            <h2 class="h2">Apache Kafka</h2>
            <div class="case-study-subsection">
              <h3>What is Kafka?</h3>
              <p>
                In a traditional message queue, events are read and then removed from the
                queue. An alternative approach is log-based message queues, which persist
                events to a log. Among log-based message queues, Kafka is the most popular
                – over 80% of Fortune 100 companies use Kafka as part of their architecture[2].
                Kafka is designed for parallelism and scalability and maintains the intended
                decoupling of an EDA. In Kafka, events are called messages.
              </p>
            </div>

            <div class="case-study-subsection">
              <h3>How Does Kafka Work?</h3>
              <figure>
                <img
                  src="assets/images/case-study/kafka.png"
                  class="case-study-image"
                />
              </figure>
              <p>
                Typically, when talking about Kafka, we are actually referring to a Kafka cluster
                - a cluster is comprised of several servers, referred to as brokers, working in 
                conjunction. A broker receives messages from producers, persists them, and makes 
                them available to consumers. 
              </p>
              <p>
                Topics are named identifiers used to group messages together. Topics, in turn,
                are broken down into partitions. To provide scalability, each partition of a given 
                topic can be hosted on a different broker. This means that a single topic can be 
                scaled horizontally across multiple brokers to provide performance beyond the ability
                of a single broker. Each instance of a consumer application can then read from a 
                partition, allowing for parallel processing of messages within a topic.
              </p>
              <p>
                Consumers are organized into consumer groups under a common group ID to enable Kafka’s
                internal load balancing. It is important to note that while a consumer instance can consume
                from more than one partition, a partition can only be consumed by a single consumer instance.
                (See below) If the number of instances is higher than the number of available partitions,
                some instances will remain inactive.
              </p>
              <figure>
                <img
                  src="assets/images/case-study/kafka.gif"
                  class="case-study-image"
                />
              </figure>
              <p>
                Internally, Kafka uses a mechanism called “commits” to track the successful processing of 
                messages. Consumer applications periodically send commits back to the Kafka cluster, indicating
                the last message they’ve successfully processed. Should a consumer instance go down, Kafka
                will have a checkpoint for where to resume message delivery.
              </p>
              <figure>
                <img
                  src="assets/images/case-study/commits.png"
                  class="case-study-image"
                />
              </figure>
            </div>
          </div>
          <!-- Section 3: Problem Description -->
          <div class="case-study-section">
            <h2 class="h2">Problem Description</h2>
            <div class="case-study-subsection">
              <h3>Head-of-Line Blocking in Kafka</h3>
              <p>
                A significant problem that can be experienced when using message queues
                is head-of-line blocking (HoLB). HoLB occurs when a message at the head 
                of the queue blocks the messages behind it. Since Kafka’s partitions are
                essentially queues, messages may block the line for two common reasons –
                poison pills and unusually slow messages. 
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Poison Pills</h3>
              <p>
                Poison pills are messages that a consumer application receives but cannot
                process. Messages can become poison pills for a host of reasons, such as
                corrupted or malformed data.
              </p>
              <h4>HoLB Due to Poison Pills</h4>
              <p>
                To better understand how poison pills cause HoLB, imagine an online vendor
                tracking orders on a website. Each order is produced to an orders topic.
                A consumer application is subscribed to this topic and needs to process
                each message so that a confirmation email for orders can be sent to customers.
                The consumer application expects to receive a message that contains an 
                integer for the product_id  field, but instead, it receives a message with
                no value for that field. With no mechanism to deal with this poison pill, 
                processing halts. This will stop all orders behind the message in question 
                even though they could be processed without problems.
              </p>
              <figure>
                <img
                  src="assets/images/case-study/poison.gif"
                  class="case-study-image"
                />
              </figure>
            </div>
            <div class="case-study-subsection">
              <h3>Non-Uniform Consumer Latency</h3>
              <p>
                Slow messages can cause non-uniform consumer latency (NUCL),
                where a consumer takes an unusually long time to process a message.
                For instance, suppose a consumer application makes a call to one of
                many external services based on the contents of a message. If one of
                these external services is sluggish, a message's processing time 
                will be unusually slow. Messages in the queue that don’t rely on the
                delayed  external service will experience an increase in processing latency.
              </p>
              <h4>HoLB Due to Non-Uniform Consumer Latency</h4>
              <p>
                To illustrate how non-uniform consumer latency causes HoLB, imagine a
                consumer application called ordersConsumer that is subscribed to the 
                orders topic. It receives the messages and, based on their product_id,
                routes them to two different external services. 
              </p>
              <ol class="numbered-list">
                <li>If the message’s product_id field has a value between 1-100, it refers
                    to electronics, and it’s sent to an external service called electronicsDB.
                </li>
                <li> If the product_id field has a value between 101-200, it refers to
                    clothing and is sent to an external service called clothingDB. 
                </li>
              </ol>
              <p>
                As ordersConsumer is pulling messages, there’s a sudden spike in latency in 
                the response from electronicsDB. The message that is currently at the head of 
                the queue represents an electronics item so the lack of response prevents the 
                consumer application from successfully processing it and all messages behind it. 
              </p>
              <figure>
                <img
                src="assets/images/case-study/nucl.gif"
                class="case-study-image"
              />
                <img />
              </figure>
              <p>
                Although all the messages behind this one are clothing items, they cannot be
                processed by ordersConsumer, even though clothingDB is functioning normally.
                Here, non-uniform consumer latency slows down the entire partition and causes HoLB.
              </p>
              <p>
                The consequences of HoLB in a message queue can range from disruptive, such as slow 
                performance, to fatal - potential crashes. An obvious solution to these issues might 
                be simply dropping messages; however, in many cases, data loss is unacceptable. For 
                our use case, an ideal solution would retain all messages.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Solution Requirements</h3>
              <p>
                Based on the problem space described, we came up with the following requirements for a solution:
              </p>
              <ol class="numbered-list">
                <li>It should be publicly available to consumer application developers.</li>
                <li>It should serve developers working in a polyglot microservices environment.</li>
                <li>Due to the strict requirement of avoiding data loss, the solution should preserve all message records.</li>
                <li>Developers should be able to integrate it smoothly into existing architectures.</li>
                <li>It should be easily deployed regardless of the user’s cloud environment (if any).</li>
              </ol>
            </div>
          </div>
          <!-- Section 4: Alternative Approaches -->
          <div class="case-study-section">
            <h2>Alternative Approaches</h2>
            <p>
              With the aforementioned requirements in mind, we extensively researched 
              existing solutions and approaches to solving HoLB. The solutions we found
              ranged from built-in Kafka configurations to service models built to support
              large Kafka deployments.
            </p>
            <div class="case-study-subsection">
              <h3>Kafka Auto-Commit</h3>
              <p>	
                By default, the Kafka consumer library sends commits back to Kafka every 5
                seconds, regardless of whether a message has been successfully processed. 
                Where data loss is not an issue, auto-commit is a reasonable solution to HoLB.
                If a problematic message is encountered, the application can simply drop the 
                message and move on. Where data loss is unacceptable, however, this approach 
                will not work.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Confluent Parallel Consumer</h3>
              <p>
                Confluent Parallel Consumer (CPC) is a Java Kafka Consumer library that seemingly
                addresses HoLB by offering an increase in parallelism beyond partition count for
                a given topic. It operates by processing messages in parallel using multiple
                threads on the consumer application’s host machine. 
              </p>
              <p>
                While CPC is an attractive solution, there were a few areas where it differed from
                our design requirements. The most obvious shortcoming for us was the fact that
                it's written in Java. In modern polyglot microservice environments, this presents
                a notable con - any developer wanting to utilize the advantages of CPC will need 
                to rewrite their applications in Java.
              </p>
              <p>
                Additionally, our requirements did not permit data loss; while setting up data loss 
                prevention with CPC is feasible, we sought a solution that came with this functionality 
                out of the box.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Kafka Workers (DoorDash)</h3>
            <p>
              DoorDash chose to leverage Kafka to help them achieve their goals of rapid
              throughput and low latency. Unfortunately, their use of Kafka introduced 
              HoLB caused by NUCL.
            </p>
            <p>
              The worker-based solution that Doordash implemented to address this problem 
              consists of a single Kafka consumer instance per partition, called a worker, 
              which pipes messages into a local queue. Task-executors within the worker
              instance then retrieve the events from this queue and process them. 
            </p>
            <p>
              This solution allows events on a single partition to be processed by multiple 
              task executors in parallel. If a single message is slow to process, it doesn’t
              impact the processing time of other messages. Other available task executors 
              can read off the local queue and process messages even though a message at the
              “head” might be slow. 
            </p>
            <p>
              While this solution solves HoLB caused by NUCL, it did not fit our design 
              requirements due to its lack of data loss prevention. According to DoorDash, 
              if a worker crashes, messages within its local queue may be lost. As previously 
              established, data loss prevention was a strict design requirement for us, making
              this approach a poor fit for our use case. 
            </p>
            </div>
            <div class="case-study-subsection">
              <h3>Consumer Proxy Model (Uber)</h3>
              <p>
                Uber sought to solve HoLB caused by NUCL and poison pills while ensuring at-least-once
                delivery since they deemed data loss intolerable. 
              </p>
              <p>
                Their solution, Consumer Proxy, solves HoLB by acting as a proxy between the Kafka
                cluster and multiple instances of the downstream consumer application. With this approach,
                messages are ingested and then processed in parallel by downstream consumer instances. 
                Consumer Proxy also uses a system of internal acknowledgments sent by consumer instances,
                indicating the successful processing of a message. Consumer Proxy only commits messages
                back to Kafka which have been acknowledged. If a message has not been acknowledged, a 
                dead-letter queue is used to store the failed message for later retrieval.
              </p>
              <p>
                Uber’s Consumer Proxy is a feature-rich solution that seems to fulfill all of our requirements.
                It eliminated HoLB due to the two causes our team was concerned with while avoiding data loss.
                That being said, Consumer Proxy is an in-house, proprietary solution that is not publicly 
                available for consumer application developers.
              </p>
  
              <figure>
                <img
                  src="assets/images/case-study/existing_solutions.png"
                  class="case-study-image"
                />
              </figure>
            </div>
          </div>
        
          <!-- Section 5: Introducing Triage -->
          <div class="case-study-section">
            <h2>Introducing Triage</h2>
            <p>
              Based on our research, none of the solutions fit all of our requirements – they either were not
              supported in multiple languages, failed to solve HoLB for both causes identified, or were not
              publicly available. We chose Uber's consumer proxy model as the basis for Triage because it
              solved both causes of HoLB and was language agnostic. As seen in the figure above, a Triage
              instance acts as a Kafka consumer proxy and passes messages to downstream consumer instances. 
            </p>
            <div class="case-study-subsection">
              <h3>How Does Triage Work?</h3>
              <p>
                Triage will subscribe to a topic on the Kafka cluster and begin consuming messages. When 
                a message is consumed, it is sent to an instance of a consumer application. This consumer 
                instance will process the message and send back a status code that reflects whether or not
                 a message has been successfully processed. Triage uses an internal system of acks and nacks
                (acknowledgments and negative acknowledgments) to identify healthy versus poison pill messages.
              </p>
              <p>
                Internally, Triage uses a commitTracker to determine which messages have been successfully 
                acknowledged and can be committed back to Kafka. Once it has done so, those records are deleted
                from the tracker. For messages that have been negatively acknowledged, Triage utilizes
                the dead-letter queue pattern to avoid data loss.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Triage Solves HoLB Caused By Poison Pills</h3>
              <p>
                When a poison pill is encountered, the consumer instance will send back a nack for that message. 
                A nack directs Triage to deliver the message record in its entirety to a DynamoDB table. Here,
                it can be accessed at any point in the future for further analysis or work. The partition will
                not be blocked, and messages can continue to be consumed uninterrupted.
              </p>
              <figure>
                <img
                src="assets/images/case-study/poison-solved.gif"
                class="case-study-image"
              />
              </figure>
            </div>
            <div class="case-study-subsection">
              <h3>Triage Solves HoLB Caused by Non-Uniform Consumer Latency</h3>
              <p>
                With Triage, If a consumer instance takes an unusually long time to process a message, 
                the partition remains unblocked. Messages can continue to be processed using other available
                consumer instances. Once the consumer instance finishes processing the slow message, it can 
                continue processing messages. 
              </p>
              <figure>
                <img
                src="assets/images/case-study/nucl-solved.gif"
                class="case-study-image"
              />
              </figure>
            </div>
            <div class="case-study-subsection">
              <h3>How Can I Use Triage?</h3>
              <h4>Deploying Triage</h4>
              <p>
                Triage can be deployed using our triage-cli command line tool, available as an
                NPM package. It offers a 2 step process that deploys Triage to AWS. You can read
                our step-by-step instructions here: <a target="_blank" href="https://github.com/team-triage/triage-cli#readme">Triage CLI</a>.
              </p>
              <h4>Connecting to Triage</h4>
              <p>
                Consumer applications can connect to Triage using our <a target="_blank" href="https://github.com/team-triage/triage-client-go#readme">thin client library</a>, currently
                offered in Go. It handles authenticating with and connecting to Triage and provides
                an easy-to-use easy interface for developers to indicate whether a message has been
                processed successfully.
              </p>
              <figure>
                <img
                src="assets/images/case-study/triage-dummy-consumer.png"
                class="case-study-image"
              />                
              <figcaption>A dummy consumer example</figcaption>
              </figure>
              </div>
          </div>
          <!-- Section 6: Triage Design Challenges  -->
          <div class="case-study-section">
            <h2>Triage Design Challenges</h2>
            <p>
              Based on our requirements for Triage, we encountered a few challenges. Below,
              we’ll present our reasoning behind the solutions we chose and how they allowed 
              us to fulfill all of our solution requirements.  
            </p>
            <div class="case-study-subsection">
              <h3>Polyglot Support</h3>
              <h4>Challenge</h4>
              <p>
                We knew we wanted Triage to be language-agnostic – a consumer application
                should be able to connect to Triage, regardless of the language it’s written
                in. To do this, we had to consider whether Triage would exist as a service between
                Kafka and the consumer or as a client library on the consumer itself. We also 
                needed to decide on a suitable network protocol.
              </p>
              <h4>Solution</h4>
              <p>
                By leveraging a service + thin client library implementation and gRPC code generation,
                 we can build out support for consumer applications written in any language with relative ease.
              </p>
              <h5>Service vs. Client Library</h5>
              <p>
                On one hand, a client library offers simplicity of implementation and testing, as well as the
                advantage of not having to add any new pieces of infrastructure to a user’s system.
                We could also expect to get buy-in from developers with less pushback, as testing a
                client library with an existing system is more manageable than integrating a new service.
              </p>
              <p>
                There were, however, some disadvantages with this approach – the most immediate being that our solution
                to addressing non-uniform consumer latency relies on parallel processing of a single partition. While,
                in theory, a client library could support multiple instances of a consumer application, a service 
                implementation is more straightforward. Even if a client library were to be designed to
                dispatch messages to multiple consumer instances, it would begin to resemble a service implementation.
              </p>
              <p>
                Another concern of ours was ease of maintainability. Within modern polyglot microservice
                environments, maintenance of client libraries written in multiple languages consumes a non-trivial
                amount of engineering hours. Changes in the Kafka version and the dependencies of the client
                libraries themselves could cause breaking changes that require time to resolve. We assumed
                that those hours could be better spent on core application logic. 
              </p>
              <button type="button" class="collapsible">Kafka configs =></button>
              <div class="content">
                <p>
                  Kafka can be difficult to work with. While the core concepts of Kafka are relatively
                  straightforward to understand, in practice, interaction with a Kafka cluster involves
                  a steep learning curve. There are over 40 configuration settings that a Kafka client
                  can specify, making setting up an optimal or even functional consumer application 
                  difficult. Uber, for example, noted that their internal Kafka team was spending about
                  half of their working hours troubleshooting for consumer application developers.
                </p>
              </div>
              <p>
                By centralizing the core functionality of Triage to a service running in the cloud and only
                utilizing a thin client library for connecting to Triage, support and maintenance become
                easier. Triage’s client library is simple – it makes an initial HTTP connection request 
                with an authentication key provided by the developer and runs a gRPC server that listens
                for incoming messages. Implementing support in additional languages for this thin client
                library is straightforward, and much of the challenge around configuring a Kafka consumer
                is abstracted away from the developer.
              </p>
              <h5>Network Protocol</h5>
              <p>
                The next decision that we faced was choosing an appropriate network protocol for communication
                with downstream applications. HTTP was an obvious consideration both for its ubiquity and ease
                of implementation; however, after further research, we felt gRPC was the better option.
              </p>
              <p>
                gRPC allows us to leverage the benefits of HTTP/2 over HTTP/1.1, specifically regarding the
                size of traffic we send and receive. HTTP/2 uses protocol buffers which are serialized and 
                emitted as binaries to achieve higher compression than HTTP/1.1, which typically uses the de-facto
                standard of JSON. Higher compression means less data to send over the network and ultimately faster
                throughput.
              </p>
              <button type="button" class="collapsible">gRPC vs JSON + gZip =></button>
              <div class="content">
                <p>
                  A counterpoint to the compression argument is the existence and growing popularity of JSON 
                  with gzip. Compression gains from protocol buffers compared JSON with gzip are less impressive;
                  however, we run into similar dependency pains mentioned in (REFERENCE Service vs. Client
                  Library Section). Each version of the thin client library we would potentially write must 
                  import its own language’s implementation of gzip. 
                </p>
              </div>
              <p>
                gRPC also makes it easy to build out support for multiple languages via out-of-the-box code
                generation. Using the same gRPC files we’ve used for our triage-client-go library, we can
                utilize a simple command-line tool to generate gRPC server and client implementations in
                all major programming languages.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Enabling Parallel Consumption</h3>
              <h4>Challenge</h4>
              <p>
                Since Triage operates by dispatching messages to several downstream consumer
                instances, we needed a way to send messages and receive responses simultaneously.
                We knew that the language we chose would play a significant role in solving this
                challenge.
              </p>
              <h4>Solution</h4>
              <p>
                By creating dedicated Goroutines for each consumer instance and synchronizing them 
                with the rest of Triage via channels, we can enable parallel consumption of a single 
                Kafka partition.
              </p>
              <h5>Language</h5>
              <p>
                We chose Go for the relative simplicity of implementing concurrency via Goroutines
                and the ease of synchronization and passing data across these Goroutines via channels. 
              </p>
              <h6>Goroutines</h6>
              <p>
                Goroutines can be thought of as non-blocking function loops that can run concurrently
                with other functions. The resource overhead of creating and running a Goroutine is
                negligible, so it’s not uncommon to see programs with thousands of Goroutines. This 
                sort of multithreaded behavior is easy to use with Go, as a generic function can be 
                turned into a Goroutine by simply prepending its invocation with the keyword go. Each 
                major component of a Triage container exists as a Goroutine, which often relies on other
                underlying Goroutines. Channels are used extensively to pass data between these components 
                and achieve synchronization where needed. 
              </p>
              <figure>
                <img src="assets/images/case-study/nogoroutine.PNG">
                <figcaption>Execution without Goroutines</figcaption>
              </figure>
              <figure>
                <img src="assets/images/case-study/withgoroutine.png">
                <figcaption>Execution with Goroutine</figcaption>
              </figure>
              <h6>Channels</h6>
              <p>
                Channels, in Go, are queue-like data structures that facilitate communication across
                processes within a Go application. Channels support passing both primitives and structs. 
                We can think of a function that writes a given value to a channel as a “sender” and a function 
                that reads said value off of the channel as a “receiver.” When a sender attempts to write
                a message but there is no receiver attempting to pull a message off of the channel,
                code execution is blocked until a receiver is ready. Similarly, if a receiver attempts
                to read a message off of the channel when there is no sender, code execution is blocked
                until a sender writes a message.
              </p>
              <figure>
                <img src="assets/images/case-study/channels.PNG">
                <figcaption>Channels Code Snippet</figcaption>
              </figure>
            </div>
            <div class="case-study-subsection">
              <h3>Ease of Deployment</h3>
              <h4>Challenge</h4>
              <p>
                We wanted to make sure that deploying Triage was as simple as possible for consumer 
                application developers. Our goal was for setup, deployment, and teardown to be painless.
              </p>
              <h4>Solution</h4>
              <p>
                By taking advantage of the AWS Cloud Development Kit (CDK) in conjunction with AWS Fargate
                on Elastic Container Service (ECS), we were able to create an automated deployment script that
                interacts with our command line tool, triage-cli.This allows users to deploy a failure-resistant
                Fargate service to AWS in just a few easy steps.
              </p>
              <h5>AWS Fargate on ECS</h5>
              <p>
                We chose AWS for its wide geographic distribution, general industry familiarity, 
                and support for containerized deployments.
              </p>
              <p>
                AWS offers services in virtually every region of the world, meaning that developers 
                looking to use Triage can deploy anywhere. We selected Fargate as the deployment 
                strategy for Triage containers, removing the overhead of provisioning and managing
                individual virtual private server instances. Instead, we could concern ourselves
                with relatively simple ECS task and service definitions.
              </p>
              <button type="button" class="collapsible">ECS Tasks and Services =></button>
              <div class="content">
                <p>
                  In our case, a task is a single container running Triage. A service is a collection 
                  of these tasks, with the number of tasks being equal to the number of partitions for a given topic.
                </p>
              </div>
              <p>
                The service definition is vital to how we guard against failure - if a Triage container
                 were to fail, it would be scrapped and another would immediately be provisioned automatically.
              </p>
              <button type="button" class="collapsible">Health Checks and Logs =></button>
              <div class="content">
                <p>
                  Using Fargate, however, doesn’t mean that we sacrifice any of the benefits
                  that come with ECS since Fargate exists on top of ECS. Health checks and
                  logs, as well as all of the infrastructure created during the deployment,
                  are available to and owned by a user, since Triage deploys using the AWS 
                  account logged into the AWS CLI.
                </p>
              </div>
              <p>
                Automated deployment is supported via Amazon’s Cloud Development Kit (CDK). 
                Manually deploying Triage would require an understanding of cloud-based networking
                that would take up significant overhead. Amazon’s CDK abstracts that away – instead
                of having to set up the dozens of individual entities that must be provisioned and 
                interconnected for a working cloud deployment, we were able to use ready-made templates
                provided by CDK for a straightforward deployment script.
              </p>
              <h5>Triage CLI</h5>
              <p>
                We created triage-cli to interact with the deployment script created with AWS CDK. This allows 
                us to interpolate user-specific configuration details into the script and deploy using just two
                commands – triage init and triage deploy.
              </p>
            </div>
          </div>
          <!-- Section 7: Implementation-->
          <div class="case-study-section">
            <h2>Implementation</h2>
            <figure>
              <img
              src="assets/images/case-study/application_logic.png"
              class="case-study-image"
            />
            </figure>
            <p>
              Having found solutions to our design challenges, our next step in developing Triage was 
              implementation. In this section, we will discuss the components that make up the application
              logic of a Triage container as well as provide a brief overview of how the Triage Client 
              interacts with it. We will address implementation with the following subsections:
              <ol class="numbered-list">
                <li><strong>Message Flow</strong> - How Triage pulls messages from Kafka and sends them to consumers</li>
                <li><strong>Consumer Instances</strong> - How a consumer instance receives messages from Triage and responds</li>
                <li><strong>Handling Acks/Nacks</strong> - How Triage handles these responses</li>
                <li><strong>Commits</strong> - How Triage handles commits</li>
              </ol>
            </p>
            <div class="case-study-subsection">
              <h3>Message Flow</h3>
              <p>
                We will start by describing the flow of messages from Kafka, through Triage,
                to downstream consumer instances.
              </p>
              <h4>Fetcher</h4>
              <p>
                The <span class="snippet">Fetcher</span> component is an instance of a Kafka consumer – it periodically polls Kafka for 
                messages. It then writes these messages to the <span class="snippet">Commit Tracker</span> component and sends them to 
                the messages channel. We will discuss the <span class="snippet">Commit Tracker</span> in a later section - for now, it is
                sufficient to know that a copy of a message ingested by Triage is stored in a hashmap, 
                and a reference to this message is sent over the messages channel.
              </p>
              <h4>Consumer Manager</h4>
              <p>
                At this point in the flow, messages from Kafka are sitting in the messages channel and are
                ready to be processed. The <span class="snippet">Consumer Manager</span> component runs a simple HTTP server that listens
                for incoming requests from consumer instances. After authenticating a request, the <span class="snippet">Consumer
                Manager</span> parses the consumer instance’s network address and writes it to the <span class="snippet">newConsumers</span> channel.
              </p>
              <p>
                To recap, at this point in the flow, we have messages from Kafka in a messages channel and
                network addresses to send them to in a <span class="snippet">newConsumers</span> channel. 
              </p>
              <h4>Dispatch</h4>
              <p>
                Triage’s <span class="snippet">Dispatch</span> component is responsible for getting messages from within Triage to the
                consumer instances. We can think of <span class="snippet">Dispatch</span> as a looping function that waits for network
                addresses on the <span class="snippet">newConsumers</span> channel. When it receives a network address, it uses it to
                instantiate a gRPC client – think of this as a simple agent to make network calls. When
                this client is created, a connection is established between the client and the consumer
                at the network address.
              </p>
              <h4>senderRoutine</h4>
              <p>
                Dispatch then calls a function called <span class="snippet">senderRoutine</span>, passing it the gRPC client as a
                parameter. <span class="snippet">senderRoutine</span> is invoked as a Goroutine, ensuring that when <span class="snippet">Dispatch</span> loops
                and listens for the next network address, <span class="snippet">senderRoutine</span> continues to run in the background.
              </p>
              <p>
                <span class="snippet">senderRoutine</span> is essentially a for loop. First, a message is pulled off of the messages
                channel. The gRPC client passed to <span class="snippet">senderRoutine</span> is then used to send this message over
                the network to the consumer instance. The senderRoutine now waits for a response.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Consumer Instances</h3>
              <p>
                Having discussed how messages get from Kafka to consumer instances, we will now
                discuss how consumer instances receive messages from and respond to Triage.
              </p>
              <p>
                Consumer applications interact with Kafka using the Triage Client. The client library is responsible for the following:
              </p>
              <ol class="numbered-list">
                <li>Providing a convenience method to send an HTTP request to Triage</li>
                <li>Accepting a message handler</li>
                <li>Running a gRPC Server</li>
              </ol>
              <p>
                We have already covered the HTTP request – we will now examine message handlers and gRPC servers.
              </p>
              <h4>Message Handler</h4>
              <p>
                Developers first pass the client library a message handler function – the message handler 
                should accept a Kafka message as a parameter, process the message, and then return either a
                positive or negative integer based on the outcome of the processing. This integer is how 
                consumer application developers can indicate whether a message has or has not been successfully
                processed.
              </p>
              <h4>gRPC Server</h4>
              <p>
                When the consumer application is started, it runs a gRPC server that listens on a dedicated port.
                When the server receives a message from Triage, it invokes the message handler, with the message
                as an argument. If the message handler returns a positive integer, it indicates that the message
                was successfully processed, and a positive acknowledgment or <span class="snippet">ack</span> is sent back to Triage. If
                the handler returns a negative integer, it indicates that the message was not processed 
                successfully, and a negative acknowledgment or <span class="snippet">nack</span>is sent to Triage.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Handling Acks/Nacks</h3>
              <figure>
                <img
                src="assets/images/case-study/ack-nack.gif"
                class="case-study-image"
              />
              </figure>
              <p>	So far, we have covered how messages get from Kafka, through Triage, and to consumer instances.
                 We then explained how Triage’s client library manages receiving these messages and sending
                responses. We will now cover how Triage handles these responses.
              </p>
              <h4>senderRoutine</h4>
              <p>
                As discussed in the message flow section, after sending a message to a consumer instance,
                <span class="snippet">senderRoutine</span> waits for a response. When a response is received, <span class="snippet">senderRoutine</span> creates an 
                <span class="snippet">acknowledgment</span> struct. The struct has two fields - <span class="snippet">Status</span> and <span class="snippet">Message</span>
                The <span class="snippet">Status</span> field of the acknowledgment struct indicates either a positive or negative acknowledgment. If the
                response was a <span class="snippet">nack</span>, a reference to the message is saved under the <span class="snippet">Message</span> field of the 
                struct.  For <span class="snippet">acked</span> messages, the Message field can be nil. Finally, before looping to send
                another message, senderRoutine places the acknowledgment struct on the acknowledgments channel. 
              </p>
              <h4>Filter</h4>
              <p>
                A component called <span class="snippet">Filter</span> listens on the <span class="snippet">acknowledgments</span> channel. It pulls <span class="snippet">acknowledgment</span>
                structs off the channel and performs one of two actions based on whether the struct represents 
                an <span class="snippet">ack</span> or a <span class="snippet">nack</span>. For <span class="snippet">acks</span>, <span class="snippet">Filter</span>
                immediately updates the <span class="snippet">Commit Tracker</span>’s hashmap. We
                will discuss the <span class="snippet">Commit Tracker</span>’s hashmap in the next section - for now, it is enough to know 
                that an <span class="snippet">ack</span> means we can mark the entry representing the message in the hashmap as acknowledged. 
              </p>
              <p>
                For <span class="snippet">nacks</span>, however, the hashmap cannot be updated immediately - we have a bad message and need 
                to ensure it is stored somewhere before moving on. <span class="snippet">Filter</span> places this negative acknowledgment in
                the <span class="snippet">deadLetters</span> channel.
              </p>
              <h4>Reaper</h4>
              <p>	
                A component called <span class="snippet">Reaper</span> listens on this <span class="snippet">deadLetters</span> channel. It makes an API call to DynamoDB,
                attempting to write the faulty message to a table. Once confirmation is received from DynamoDB that
                the write was successful, the entry representing the message in <span class="snippet">Commit Tracker</span>’s hashmap can be marked 
                as acknowledged.
              </p>
              <p>
                At this point, we have covered how messages get from Kafka, through Triage, to consumer instances. We
                have also covered how consumer instances process these messages, send responses back to Triage, 
                and how Triage handles these responses. 
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Commits</h3>
              <p>
                We will now cover the <span class="snippet">Commit Tracker</span> component and how it allows us to manage commits back to Kafka effectively.
              </p>
              <h4>Commit Hashmap</h4>
              <p>As discussed in the message flow section, as messages are ingested by Triage,
                 we store a reference to them in a hashmap. The hashmap’s keys are the offsets 
                 of the messages, and the values are a custom struct called <span class="snippet">CommitStore</span>. The 
                 <span class="snippet">commitStore</span> struct has two fields - <span class="snippet">Value</span> and <span class="snippet">Message</span>. 
                 The <span class="snippet">Message</span> field stores a reference to a specific Kafka message; the <span class="snippet">Value</span> field stores whether
                 or not Triage has received a response for this message.
              </p>
              <p>
                Previously, we mentioned that the <span class="snippet">Filter</span> and <span class="snippet">Reaper</span> components marked messages 
                in the hashmap as acknowledged. More specifically, they were updating the <span class="snippet">Value</span>
                 field. Because messages that are <span class="snippet">nacked</span> are stored in DynamoDB for processing 
                 at a later time, we can think of them as “processed,” at least with respect to 
                 calculating which offset to commit back to Kafka.

              </p>
              <h4>Commit Calculator</h4>
              <figure>
                <img
                src="assets/images/case-study/commit-calculator.gif"
                class="case-study-image"
              />
              </figure>

              <p>
                To calculate this offset, a component called <span class="snippet">CommitCalculator</span> periodically runs 
                in the background.  To be efficient with our commits, we want to commit the highest
                offset possible since Kafka will implicitly commit all messages below our committed
                offset. For example, if there are 100 messages in a partition and we commit 
                offset 50, Kafka will consider offsets 0-49 as “committed.”
              </p>
              <p>
                Extending this example, let us assume that these 100 messages are currently stored
                in the hashmap. If we have received acknowledgments for offsets 0 through 48 – 
                and offset 50 –  but have not yet received an acknowledgment for offset 49, we 
                cannot commit offset 50 since that would imply that 49 has been successfully processed.
                Instead, we could only commit offset 48 and would have to wait for the acknowledgment
                for 49 before committing 50. (diagram here is definitely needed)
              </p>
              <p>
                In other words, <span class="snippet">Commit Tracker</span> commits the greatest offset for which all previous 
                messages have also been acknowledged.  Once this offset is determined, Triage sends
                a commit back to the Kafka cluster and awaits confirmation that the commit was
                successful. Finally, all entries up to and including the offset of the message 
                just committed are removed from <span class="snippet">Commit Tracker</span>’s hashmap. 
              </p>
              <p>
                Having covered commits, we have arrived at the end of a message’s life cycle within a system using Triage.

              </p>
            </div>

          </div>
          <!-- Section 8: Future Work-->
          <div class="case-study-section">
            <h2>Future Work</h2>
            <p>
              Below are the improvements we would like to implement in the future.
            </p>
            <div class="case-study-subsection">
              <h3>Language Support for the Triage Client Library</h3>
              <p>
                It would be beneficial to have our client library written in other widely used languages, 
                such as JavaScript and Ruby. This would enable more developers to use Triage and cement 
                our decision to use a service + thin client implementation.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Cause of Failure Field</h3>
              <p>
                Giving the consumer application developer the ability to add a reason for failure to poison
                pill messages would help them to identify errors and analyze them. It would be a custom field 
                supplied by the developer that would be stored with the message record under a different column
                in the DynamoDB table.
              </p>
            </div>
            <div class="case-study-subsection">
              <h3>Notifications for Poison Pills</h3>
              <p>
                We believers developers could benefit from notifications when poison pills are written to DynamoDB. 
                Having these notifications integrated with a platform like Slack could serve as an alarm to 
                avoid further issues. One approach would be to use AWS Cloudwatch to implement this feature.
              </p>
            </div>
          </div>   
          <!-- Section 10 -->
          <div class="case-study-section team">
            <h2><a href="./team.html">The Triage Team</a></h2>
            <br />
            <br />
            <div class="section team-section">
              <div class="container">
                <div
                  data-duration-in="300"
                  data-duration-out="100"
                  class="tabs w-tabs"
                >
                  <div
                    data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a"
                    style="
                      transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1)
                        rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg);
                      transform-style: preserve-3d;
                      opacity: 0;
                    "
                    class="tabs-content w-tab-content"
                  >
                    <div>
                      <div class="team-grid">
                        <div class="team-member-wrap">
                          <img
                            src="assets/images/team/aashish.jpg"
                            loading="lazy"
                            alt=""
                          />
                          <div class="team-member-info">
                            <div class="team-member-name">Aashish Balaji</div>
                            <div class="team-member-location">Toronto, Canada</div>
                          </div>
                          <ul class="team-member-icons">
                            <li>
                              <a href="mailto:aashish.balaji1@live.com" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-envelope"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a href="https://www.imaashish.com/" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-globe"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a
                                href=" https://www.linkedin.com/in/aashish-balaji-57a087249/"
                                target="_blank"
                              >
                                <span class="team-member-icon">
                                  <i class="fab fa-linkedin"></i>
                                </span>
                              </a>
                            </li>
                          </ul>
                        </div>
                        <div class="team-member-wrap">
                          <img
                            src="assets/images/team/aryan.jpg"
                            loading="lazy"
                            alt=""
                          />
                          <div class="team-member-info">
                            <div class="team-member-name">Aryan Binazir</div>
                            <div class="team-member-location">Chapel Hill, NC</div>
                          </div>
                          <ul class="team-member-icons">
                            <li>
                              <a
                                href="mailto:abinazir@gmail.com"
                                target="_blank"
                              >
                                <span class="team-member-icon">
                                  <i class="fas fa-envelope"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a href="https://www.aryanbinazir.dev/" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-globe"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a
                                href="https://www.linkedin.com/in/aryanbinazir/"
                                target="_blank"
                              >
                                <span class="team-member-icon">
                                  <i class="fab fa-linkedin"></i>
                                </span>
                              </a>
                            </li>
                          </ul>
                        </div>
                        <div class="team-member-wrap">
                          <img src="assets/images/team/jordan.png" loading="lazy" alt="" />
                          <div class="team-member-info">
                            <div class="team-member-name">Jordan Swartz</div>
                            <div class="team-member-location">Los Angeles, CA</div>
                          </div>
                          <ul class="team-member-icons">
                            <li>
                              <a href="mailto:jordanLswartz@gmail.com" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-envelope"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a href="https://www.jordanLswartz.com" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-globe"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a href="https://www.linkedin.com/in/jordanlswartz/" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fab fa-linkedin"></i>
                                </span>
                              </a>
                            </li>
                          </ul>
                        </div>
                        <div class="team-member-wrap">
                          <img
                            src="assets/images/team/michael.jpg"
                            loading="lazy"
                            alt=""
                          />
                          <div class="team-member-info">
                            <div class="team-member-name">Michael Jung</div>
                            <div class="team-member-location">San Diego, CA</div>
                          </div>
                          <ul class="team-member-icons">
                            <li>
                              <a href="mailto:themikejung@gmail.com" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-envelope"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a href="http://themikejung.com/" target="_blank">
                                <span class="team-member-icon">
                                  <i class="fas fa-globe"></i>
                                </span>
                              </a>
                            </li>
                            <li>
                              <a
                                href="https://www.linkedin.com/in/michael-jung-02724315b/"
                                target="_blank"
                              >
                                <span class="team-member-icon">
                                  <i class="fab fa-linkedin"></i>
                                </span>
                              </a>
                            </li>
                          </ul>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </article>
    </div>

    <script
      src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
      type="text/javascript"
      integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
      crossorigin="anonymous"
    ></script>
    <script
      src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
      type="text/javascript"
    ></script>
  
    <script>
      /*!
       * toc - jQuery Table of Contents Plugin
       * v0.3.2
       * http://projects.jga.me/toc/
       * copyright Greg Allen 2014
       * MIT License
       */
      !(function (a) {
        (a.fn.smoothScroller = function (b) {
          b = a.extend({}, a.fn.smoothScroller.defaults, b);
          var c = a(this);
          return (
            a(b.scrollEl).animate(
              {
                scrollTop:
                  c.offset().top - a(b.scrollEl).offset().top - b.offset,
              },
              b.speed,
              b.ease,
              function () {
                var a = c.attr("id");
                a.length &&
                  (history.pushState
                    ? history.pushState(null, null, "#" + a)
                    : (document.location.hash = a)),
                  c.trigger("smoothScrollerComplete");
              }
            ),
            this
          );
        }),
          (a.fn.smoothScroller.defaults = {
            speed: 400,
            ease: "swing",
            scrollEl: "body,html",
            offset: 0,
          }),
          a("body").on("click", "[data-smoothscroller]", function (b) {
            b.preventDefault();
            var c = a(this).attr("href");
            0 === c.indexOf("#") && a(c).smoothScroller();
          });
      })(jQuery),
        (function (a) {
          var b = {};
          (a.fn.toc = function (b) {
            var c,
              d = this,
              e = a.extend({}, jQuery.fn.toc.defaults, b),
              f = a(e.container),
              g = a(e.selectors, f),
              h = [],
              i = e.activeClass,
              j = function (b, c) {
                if (
                  e.smoothScrolling &&
                  "function" == typeof e.smoothScrolling
                ) {
                  b.preventDefault();
                  var f = a(b.target).attr("href");
                  e.smoothScrolling(f, e, c);
                }
                a("li", d).removeClass(i), a(b.target).parent().addClass(i);
              },
              k = function () {
                c && clearTimeout(c),
                  (c = setTimeout(function () {
                    for (
                      var b,
                        c = a(window).scrollTop(),
                        f = Number.MAX_VALUE,
                        g = 0,
                        j = 0,
                        k = h.length;
                      k > j;
                      j++
                    ) {
                      var l = Math.abs(h[j] - c);
                      f > l && ((g = j), (f = l));
                    }
                    a("li", d).removeClass(i),
                      (b = a("li:eq(" + g + ")", d).addClass(i)),
                      e.onHighlight(b);
                  }, 50));
              };
            return (
              e.highlightOnScroll && (a(window).bind("scroll", k), k()),
              this.each(function () {
                var b = a(this),
                  c = a(e.listType);
                g.each(function (d, f) {
                  var g = a(f);
                  h.push(g.offset().top - e.highlightOffset);
                  var i = e.anchorName(d, f, e.prefix);
                  if (f.id !== i) {
                    a("<span/>").attr("id", i).insertBefore(g);
                  }
                  var l = a("<a/>")
                      .text(e.headerText(d, f, g))
                      .attr("href", "#" + i)
                      .bind("click", function (c) {
                        a(window).unbind("scroll", k),
                          j(c, function () {
                            a(window).bind("scroll", k);
                          }),
                          b.trigger("selected", a(this).attr("href"));
                      }),
                    m = a("<li/>")
                      .addClass(e.itemClass(d, f, g, e.prefix))
                      .append(l);
                  c.append(m);
                }),
                  b.html(c);
              })
            );
          }),
            (jQuery.fn.toc.defaults = {
              container: "body",
              listType: "<ul/>",
              selectors: "h1,h2,h3",
              smoothScrolling: function (b, c, d) {
                a(b)
                  .smoothScroller({ offset: c.scrollToOffset })
                  .on("smoothScrollerComplete", function () {
                    d();
                  });
              },
              scrollToOffset: 0,
              prefix: "toc",
              activeClass: "toc-active",
              onHighlight: function () {},
              highlightOnScroll: !0,
              highlightOffset: 100,
              anchorName: function (c, d, e) {
                if (d.id.length) return d.id;
                var f = a(d)
                  .text()
                  .replace(/[^a-z0-9]/gi, " ")
                  .replace(/\s+/g, "-")
                  .toLowerCase();
                if (b[f]) {
                  for (var g = 2; b[f + g]; ) g++;
                  f = f + "-" + g;
                }
                return (b[f] = !0), e + "-" + f;
              },
              headerText: function (a, b, c) {
                return c.text();
              },
              itemClass: function (a, b, c, d) {
                return d + "-" + c[0].tagName.toLowerCase();
              },
            });
        })(jQuery);
    </script>
    <script>
      /* initialize */
      $(".toc").toc({
        selectors: "h2", //elements to use as headings
        container: "article", //element to find all selectors in
        smoothScrolling: true, //enable or disable smooth scrolling on click
        prefix: "toc", //prefix for anchor tags and class names
        highlightOnScroll: true, //add class to heading that is currently in focus
        highlightOffset: 100, //offset to trigger the next headline
      });
    </script>
    <script type="text/javascript" src="assets/scripts/logo-btn.js"></script>
  </body>
</html>
